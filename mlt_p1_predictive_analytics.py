# -*- coding: utf-8 -*-
"""MLT_P1_Predictive_Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pk2khNCdrbU7AXDK-URd1yj3iJrYwRq7

# Import Library
"""

import zipfile, os, shutil
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

"""# Data Loading and Exploration"""

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d fhabibimoghaddam/meta-stock-historical-prices-and-data2

local_zip = '/content/meta-stock-historical-prices-and-data2.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

meta = pd.read_csv('META.csv')
meta.info()

meta.isnull().sum()

meta.describe()

meta.head()

# Mengonversi tipe data 'Date' ke datetime
meta['Date'] = pd.to_datetime(meta['Date'])

# Membuat grafik tren waktu menggunakan 'Adj Close'
plt.figure(figsize=(12, 6))
plt.plot(meta['Date'], meta['Adj Close'], label='Adj Close', color='blue')
plt.title('Tren Waktu Harga Saham META')
plt.xlabel('Tahun')
plt.ylabel('Harga Penutupan')
plt.legend()
plt.show()

# Memvisualisasikan distribusi data dan outlier
plt.subplots(figsize=(10,7))
sns.boxplot(data=meta).set_title("META Platforms, Inc")
plt.show()

# Hapus outlier menggunakan metode IQR
numeric_columns = meta.select_dtypes(include=['float64', 'int64']).columns
Q1 = meta[numeric_columns].quantile(0.25)
Q3 = meta[numeric_columns].quantile(0.75)
IQR = Q3 - Q1
meta = meta[~((meta[numeric_columns] < (Q1 - 1.5 * IQR)) | (meta[numeric_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]

print(meta.shape)

# Histogram fitur Numerik
meta.hist(bins=50, figsize=(20,15))
plt.show()

# Memvisualisasikan hubungan antar fitur
sns.pairplot(meta, diag_kind = 'kde')

# Memvisualisasikan matriks korelasi
plt.figure(figsize=(10, 8))
correlation_matrix = meta.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=10)

"""# Data Preprocessing"""

# Menghapus fitur yang tidak diperlukan
meta = meta.drop(['Date', 'Volume', 'Close'], axis=1)
meta.head()

meta.describe()

# Memisahkan fitur (X) dan variabel target (y)
X = meta.iloc[:, :-1].values
y = meta.iloc[:, -1].values

# Melakukan pembagian dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f'Total Dataset: {len(X)}')
print(f'Total Train Dataset: {len(X_train)}')
print(f'Total Test Dataset: {len(X_test)}')

"""# Model Selection and Training"""

# Menskalakan fitur
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Buat DataFrame untuk menyimpan metrik kinerja model
models = pd.DataFrame(columns=['train_mse', 'test_mse'], index=['SVR', 'KNN', 'GradientBoosting'])

# Fungsi untuk melakukan pencarian Grid
def grid_search(model, hyperparameters):
  results = GridSearchCV(
      model,
      hyperparameters,
      cv=5,
      verbose=1,
      n_jobs=6
  )

  return results

# Melakukan pencarian Grid untuk SVR
svr = SVR()
hyperparameters = {
    'kernel': ['rbf'],
    'C': [0.001, 0.01, 0.1, 10, 100, 1000],
    'gamma': [0.3, 0.03, 0.003, 0.0003]
}

svr_search = grid_search(svr, hyperparameters)
svr_search.fit(X_train, y_train)
print(svr_search.best_params_)
print(svr_search.best_score_)

# Melakukan pencarian grid untuk Gradient Boosting
gradient_boost = GradientBoostingRegressor()
hyperparameters = {
    'learning_rate': [0.01, 0.001, 0.0001],
    'n_estimators': [250, 500, 750, 1000],
    'criterion': ['friedman_mse', 'squared_error']
}

gradient_boost_search = grid_search(gradient_boost, hyperparameters)
gradient_boost_search.fit(X_train, y_train)
print(gradient_boost_search.best_params_)
print(gradient_boost_search.best_score_)

# Melakukan pencarian grid untuk KNN
knn = KNeighborsRegressor()
hyperparameters = {
    'n_neighbors': range(1, 10)
}

knn_search = grid_search(knn, hyperparameters)
knn_search.fit(X_train, y_train)
print(knn_search.best_params_)
print(knn_search.best_score_)

# Inisialisasi dan latih model SVR
svr = SVR(C=10, gamma=0.3, kernel='rbf')
svr.fit(X_train, y_train)

# Inisialisasi dan latih model Gradient Boosting
gradient_boost = GradientBoostingRegressor(criterion='squared_error', learning_rate=0.01, n_estimators=1000)
gradient_boost.fit(X_train, y_train)

# Inisialisasi dan latih model KNN
knn = KNeighborsRegressor(n_neighbors=9)
knn.fit(X_train, y_train)

"""# Model Evaluation"""

# Buat kamus untuk menyimpan model
model_dict = {
    'SVR': svr,
    'GradientBoosting': gradient_boost,
    'KNN': knn,
}

# Evaluasi kinerja setiap model pada data pelatihan dan pengujian
for name, model in model_dict.items():
  models.loc[name, 'train_mse'] = mean_squared_error(y_train, model.predict(X_train))
  models.loc[name, 'test_mse'] = mean_squared_error(y_test, model.predict(X_test))

models.head()

"""# Visualization of Model Comparison


"""

# Plot perbandingan kinerja model
fig, ax = plt.subplots()
models.sort_values(by='test_mse', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""# Prediction"""

# Pilih subset data uji untuk prediksi
num_rows_to_predict = 1
X_test_subset = X_test[:num_rows_to_predict, :]
y_test_subset = y_test[:num_rows_to_predict]

# Buat kamus untuk menyimpan nilai benar dan prediksi
pred_dict = {'y_true': y_test_subset}

# Buat prediksi dari setiap model
for name, model in model_dict.items():
    predictions = model.predict(X_test_subset).round(1)
    pred_dict['prediksi_' + name] = predictions

pd.DataFrame(pred_dict)